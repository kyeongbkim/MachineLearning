{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d240185b-c842-41a7-ab9a-7d690a565e14",
   "metadata": {},
   "source": [
    "# Downloading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b1bdca-f66b-4516-b1e8-393e3cbc251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    # Download the dataset\n",
    "    !mkdir data\n",
    "    !wget http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip -P ./data/\n",
    "    !wget http://images.cocodataset.org/zips/train2014.zip -P ./data/\n",
    "    !wget http://images.cocodataset.org/zips/val2014.zip -P ./data/\n",
    "    !unzip ./data/captions_train-val2014.zip -d ./data/\n",
    "    !unzip ./data/train2014.zip -d ./data/\n",
    "    !unzip ./data/val2014.zip -d ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1599c2-57b7-4d54-a9d1-ff5b84168ef4",
   "metadata": {},
   "source": [
    "# Processing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bedc635f-8e70-4190-a20a-c3a0e5503c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./data/vocab.pkl'):\n",
    "    # build a vocabulary list\n",
    "    !python build_vocab.py   \n",
    "\n",
    "if not os.path.exists('./data/resized2014'):\n",
    "    # Resize all the images to bring them to shape 224x224\n",
    "    !python resize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca1ce0e-534d-45b4-a639-afbf837d77a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff92739-0da1-43df-bbfb-74f4e210ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from data_loader import get_loader\n",
    "from build_vocab import Vocabulary\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6e25701-1a5f-4a3e-9f0f-21e53eaf1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args = SimpleNamespace(model_path='./models/',\n",
    "                       vocab_path='data/vocab.pkl',\n",
    "                       image_dir='data/resized2014',\n",
    "                       caption_path='data/annotations/captions_train2014.json',\n",
    "                       crop_size=224,\n",
    "                       log_step=10,\n",
    "                       save_step=1000,\n",
    "                       embed_size=256,\n",
    "                       hidden_size=512,\n",
    "                       num_layers=2,\n",
    "                       num_epochs=2,\n",
    "                       batch_size=128,\n",
    "                       num_workers=2,\n",
    "                       learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc446db-3044-4082-a9cd-23a1dbdc0dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Create model directory\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "# Image preprocessing, normalization for the pretrained resnet\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(args.crop_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Load vocabulary wrapper\n",
    "with open(args.vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# Build data loader\n",
    "data_loader = get_loader(args.image_dir, args.caption_path, vocab,\n",
    "                         transform, args.batch_size,\n",
    "                         shuffle=True, num_workers=args.num_workers)\n",
    "\n",
    "# Build the models\n",
    "encoder = EncoderCNN(args.embed_size).to(device)\n",
    "decoder = DecoderRNN(args.embed_size, args.hidden_size, len(vocab), args.num_layers).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bf44c8-f72d-403e-9c9b-91dc8a84ac90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(model_path='./models/', vocab_path='data/vocab.pkl', image_dir='data/resized2014', caption_path='data/annotations/captions_train2014.json', crop_size=224, log_step=10, save_step=1000, embed_size=256, hidden_size=512, num_layers=2, num_epochs=2, batch_size=128, num_workers=2, learning_rate=0.001) \n",
      "\n",
      ">>> Begin epoch 0\n",
      "Epoch [0/2], Step [0/3236], Loss: 9.2067, Perplexity: 9964.0078\n",
      "Epoch [0/2], Step [10/3236], Loss: 5.8962, Perplexity: 363.6424\n",
      "Epoch [0/2], Step [20/3236], Loss: 5.4293, Perplexity: 227.9847\n",
      "Epoch [0/2], Step [30/3236], Loss: 5.3328, Perplexity: 207.0119\n",
      "Epoch [0/2], Step [40/3236], Loss: 5.0016, Perplexity: 148.6541\n",
      "Epoch [0/2], Step [50/3236], Loss: 4.8911, Perplexity: 133.0957\n",
      "Epoch [0/2], Step [60/3236], Loss: 4.7773, Perplexity: 118.7891\n",
      "Epoch [0/2], Step [70/3236], Loss: 4.6424, Perplexity: 103.7909\n",
      "Epoch [0/2], Step [80/3236], Loss: 4.5284, Perplexity: 92.6142\n",
      "Epoch [0/2], Step [90/3236], Loss: 4.6010, Perplexity: 99.5826\n",
      "Epoch [0/2], Step [100/3236], Loss: 4.5523, Perplexity: 94.8489\n",
      "Epoch [0/2], Step [110/3236], Loss: 4.4377, Perplexity: 84.5784\n",
      "Epoch [0/2], Step [120/3236], Loss: 4.4190, Perplexity: 83.0100\n",
      "Epoch [0/2], Step [130/3236], Loss: 4.4086, Perplexity: 82.1565\n",
      "Epoch [0/2], Step [140/3236], Loss: 4.2349, Perplexity: 69.0560\n",
      "Epoch [0/2], Step [150/3236], Loss: 4.2191, Perplexity: 67.9756\n",
      "Epoch [0/2], Step [160/3236], Loss: 4.0929, Perplexity: 59.9127\n",
      "Epoch [0/2], Step [170/3236], Loss: 4.0300, Perplexity: 56.2595\n",
      "Epoch [0/2], Step [180/3236], Loss: 3.9356, Perplexity: 51.1936\n",
      "Epoch [0/2], Step [190/3236], Loss: 3.9865, Perplexity: 53.8680\n",
      "Epoch [0/2], Step [200/3236], Loss: 3.9588, Perplexity: 52.3941\n",
      "Epoch [0/2], Step [210/3236], Loss: 3.9223, Perplexity: 50.5188\n",
      "Epoch [0/2], Step [220/3236], Loss: 3.8075, Perplexity: 45.0372\n",
      "Epoch [0/2], Step [230/3236], Loss: 3.9059, Perplexity: 49.6935\n",
      "Epoch [0/2], Step [240/3236], Loss: 3.7464, Perplexity: 42.3700\n",
      "Epoch [0/2], Step [250/3236], Loss: 3.6845, Perplexity: 39.8238\n",
      "Epoch [0/2], Step [260/3236], Loss: 3.8518, Perplexity: 47.0776\n",
      "Epoch [0/2], Step [270/3236], Loss: 3.7028, Perplexity: 40.5597\n",
      "Epoch [0/2], Step [280/3236], Loss: 3.6342, Perplexity: 37.8706\n",
      "Epoch [0/2], Step [290/3236], Loss: 3.4822, Perplexity: 32.5311\n",
      "Epoch [0/2], Step [300/3236], Loss: 3.8810, Perplexity: 48.4717\n",
      "Epoch [0/2], Step [310/3236], Loss: 3.6961, Perplexity: 40.2899\n",
      "Epoch [0/2], Step [320/3236], Loss: 3.5750, Perplexity: 35.6957\n",
      "Epoch [0/2], Step [330/3236], Loss: 3.6545, Perplexity: 38.6464\n",
      "Epoch [0/2], Step [340/3236], Loss: 3.5153, Perplexity: 33.6252\n",
      "Epoch [0/2], Step [350/3236], Loss: 3.7018, Perplexity: 40.5189\n",
      "Epoch [0/2], Step [360/3236], Loss: 3.5221, Perplexity: 33.8545\n",
      "Epoch [0/2], Step [370/3236], Loss: 3.4521, Perplexity: 31.5673\n",
      "Epoch [0/2], Step [380/3236], Loss: 3.3987, Perplexity: 29.9247\n",
      "Epoch [0/2], Step [390/3236], Loss: 3.4813, Perplexity: 32.5009\n",
      "Epoch [0/2], Step [400/3236], Loss: 3.4830, Perplexity: 32.5563\n",
      "Epoch [0/2], Step [410/3236], Loss: 3.4999, Perplexity: 33.1132\n",
      "Epoch [0/2], Step [420/3236], Loss: 3.5094, Perplexity: 33.4282\n",
      "Epoch [0/2], Step [430/3236], Loss: 3.3345, Perplexity: 28.0654\n",
      "Epoch [0/2], Step [440/3236], Loss: 3.3537, Perplexity: 28.6080\n",
      "Epoch [0/2], Step [450/3236], Loss: 3.3024, Perplexity: 27.1764\n",
      "Epoch [0/2], Step [460/3236], Loss: 3.2502, Perplexity: 25.7959\n",
      "Epoch [0/2], Step [470/3236], Loss: 3.3996, Perplexity: 29.9507\n",
      "Epoch [0/2], Step [480/3236], Loss: 3.2341, Perplexity: 25.3830\n",
      "Epoch [0/2], Step [490/3236], Loss: 3.4135, Perplexity: 30.3719\n",
      "Epoch [0/2], Step [500/3236], Loss: 3.4339, Perplexity: 30.9961\n",
      "Epoch [0/2], Step [510/3236], Loss: 3.3415, Perplexity: 28.2603\n",
      "Epoch [0/2], Step [520/3236], Loss: 3.3415, Perplexity: 28.2625\n",
      "Epoch [0/2], Step [530/3236], Loss: 3.1879, Perplexity: 24.2378\n",
      "Epoch [0/2], Step [540/3236], Loss: 3.1510, Perplexity: 23.3602\n",
      "Epoch [0/2], Step [550/3236], Loss: 3.2004, Perplexity: 24.5424\n",
      "Epoch [0/2], Step [560/3236], Loss: 3.1037, Perplexity: 22.2810\n",
      "Epoch [0/2], Step [570/3236], Loss: 3.2020, Perplexity: 24.5805\n",
      "Epoch [0/2], Step [580/3236], Loss: 3.2104, Perplexity: 24.7899\n",
      "Epoch [0/2], Step [590/3236], Loss: 3.1451, Perplexity: 23.2229\n",
      "Epoch [0/2], Step [600/3236], Loss: 3.1294, Perplexity: 22.8612\n",
      "Epoch [0/2], Step [610/3236], Loss: 3.0585, Perplexity: 21.2955\n",
      "Epoch [0/2], Step [620/3236], Loss: 3.0971, Perplexity: 22.1348\n",
      "Epoch [0/2], Step [630/3236], Loss: 3.1726, Perplexity: 23.8683\n",
      "Epoch [0/2], Step [640/3236], Loss: 3.1852, Perplexity: 24.1726\n",
      "Epoch [0/2], Step [650/3236], Loss: 2.9749, Perplexity: 19.5884\n",
      "Epoch [0/2], Step [660/3236], Loss: 3.1565, Perplexity: 23.4876\n",
      "Epoch [0/2], Step [670/3236], Loss: 2.9971, Perplexity: 20.0282\n",
      "Epoch [0/2], Step [680/3236], Loss: 3.1535, Perplexity: 23.4179\n",
      "Epoch [0/2], Step [690/3236], Loss: 3.1133, Perplexity: 22.4956\n",
      "Epoch [0/2], Step [700/3236], Loss: 3.1142, Perplexity: 22.5155\n",
      "Epoch [0/2], Step [710/3236], Loss: 3.0515, Perplexity: 21.1478\n",
      "Epoch [0/2], Step [720/3236], Loss: 3.1514, Perplexity: 23.3693\n",
      "Epoch [0/2], Step [730/3236], Loss: 2.9769, Perplexity: 19.6264\n",
      "Epoch [0/2], Step [740/3236], Loss: 2.9683, Perplexity: 19.4582\n",
      "Epoch [0/2], Step [750/3236], Loss: 3.0510, Perplexity: 21.1358\n",
      "Epoch [0/2], Step [760/3236], Loss: 2.9980, Perplexity: 20.0463\n",
      "Epoch [0/2], Step [770/3236], Loss: 3.0597, Perplexity: 21.3219\n",
      "Epoch [0/2], Step [780/3236], Loss: 2.9653, Perplexity: 19.4007\n",
      "Epoch [0/2], Step [790/3236], Loss: 3.1902, Perplexity: 24.2933\n",
      "Epoch [0/2], Step [800/3236], Loss: 3.0105, Perplexity: 20.2981\n",
      "Epoch [0/2], Step [810/3236], Loss: 2.8292, Perplexity: 16.9316\n",
      "Epoch [0/2], Step [820/3236], Loss: 2.9216, Perplexity: 18.5710\n",
      "Epoch [0/2], Step [830/3236], Loss: 2.9413, Perplexity: 18.9413\n",
      "Epoch [0/2], Step [840/3236], Loss: 2.9996, Perplexity: 20.0766\n",
      "Epoch [0/2], Step [850/3236], Loss: 2.9665, Perplexity: 19.4241\n",
      "Epoch [0/2], Step [860/3236], Loss: 2.8742, Perplexity: 17.7113\n",
      "Epoch [0/2], Step [870/3236], Loss: 3.0502, Perplexity: 21.1194\n",
      "Epoch [0/2], Step [880/3236], Loss: 2.9528, Perplexity: 19.1605\n",
      "Epoch [0/2], Step [890/3236], Loss: 2.7635, Perplexity: 15.8550\n",
      "Epoch [0/2], Step [900/3236], Loss: 2.7454, Perplexity: 15.5712\n",
      "Epoch [0/2], Step [910/3236], Loss: 2.9447, Perplexity: 19.0042\n",
      "Epoch [0/2], Step [920/3236], Loss: 2.8392, Perplexity: 17.1024\n",
      "Epoch [0/2], Step [930/3236], Loss: 2.8079, Perplexity: 16.5743\n",
      "Epoch [0/2], Step [940/3236], Loss: 2.9172, Perplexity: 18.4887\n",
      "Epoch [0/2], Step [950/3236], Loss: 2.8089, Perplexity: 16.5914\n",
      "Epoch [0/2], Step [960/3236], Loss: 2.8132, Perplexity: 16.6639\n",
      "Epoch [0/2], Step [970/3236], Loss: 2.8464, Perplexity: 17.2249\n",
      "Epoch [0/2], Step [980/3236], Loss: 2.7104, Perplexity: 15.0354\n",
      "Epoch [0/2], Step [990/3236], Loss: 2.7978, Perplexity: 16.4079\n",
      "Epoch [0/2], Step [1000/3236], Loss: 2.7336, Perplexity: 15.3881\n",
      "Epoch [0/2], Step [1010/3236], Loss: 2.7206, Perplexity: 15.1892\n",
      "Epoch [0/2], Step [1020/3236], Loss: 2.6067, Perplexity: 13.5542\n",
      "Epoch [0/2], Step [1030/3236], Loss: 2.7857, Perplexity: 16.2117\n",
      "Epoch [0/2], Step [1040/3236], Loss: 2.8415, Perplexity: 17.1413\n",
      "Epoch [0/2], Step [1050/3236], Loss: 2.7848, Perplexity: 16.1968\n",
      "Epoch [0/2], Step [1060/3236], Loss: 2.6538, Perplexity: 14.2077\n",
      "Epoch [0/2], Step [1070/3236], Loss: 2.9718, Perplexity: 19.5264\n",
      "Epoch [0/2], Step [1080/3236], Loss: 2.6576, Perplexity: 14.2624\n",
      "Epoch [0/2], Step [1090/3236], Loss: 2.9834, Perplexity: 19.7555\n",
      "Epoch [0/2], Step [1100/3236], Loss: 2.7315, Perplexity: 15.3552\n",
      "Epoch [0/2], Step [1110/3236], Loss: 2.7293, Perplexity: 15.3228\n",
      "Epoch [0/2], Step [1120/3236], Loss: 2.9096, Perplexity: 18.3488\n",
      "Epoch [0/2], Step [1130/3236], Loss: 2.7140, Perplexity: 15.0902\n",
      "Epoch [0/2], Step [1140/3236], Loss: 2.7872, Perplexity: 16.2359\n",
      "Epoch [0/2], Step [1150/3236], Loss: 2.7133, Perplexity: 15.0791\n",
      "Epoch [0/2], Step [1160/3236], Loss: 2.7283, Perplexity: 15.3072\n",
      "Epoch [0/2], Step [1170/3236], Loss: 2.8031, Perplexity: 16.4960\n",
      "Epoch [0/2], Step [1180/3236], Loss: 2.6791, Perplexity: 14.5718\n",
      "Epoch [0/2], Step [1190/3236], Loss: 2.6405, Perplexity: 14.0205\n",
      "Epoch [0/2], Step [1200/3236], Loss: 2.6634, Perplexity: 14.3443\n",
      "Epoch [0/2], Step [1210/3236], Loss: 2.8009, Perplexity: 16.4603\n",
      "Epoch [0/2], Step [1220/3236], Loss: 2.6852, Perplexity: 14.6612\n",
      "Epoch [0/2], Step [1230/3236], Loss: 2.6328, Perplexity: 13.9133\n",
      "Epoch [0/2], Step [1240/3236], Loss: 2.7967, Perplexity: 16.3898\n",
      "Epoch [0/2], Step [1250/3236], Loss: 2.5595, Perplexity: 12.9295\n",
      "Epoch [0/2], Step [1260/3236], Loss: 2.6519, Perplexity: 14.1812\n",
      "Epoch [0/2], Step [1270/3236], Loss: 2.7041, Perplexity: 14.9405\n",
      "Epoch [0/2], Step [1280/3236], Loss: 2.6849, Perplexity: 14.6565\n",
      "Epoch [0/2], Step [1290/3236], Loss: 2.6198, Perplexity: 13.7327\n",
      "Epoch [0/2], Step [1300/3236], Loss: 2.6862, Perplexity: 14.6753\n",
      "Epoch [0/2], Step [1310/3236], Loss: 2.5216, Perplexity: 12.4489\n",
      "Epoch [0/2], Step [1320/3236], Loss: 2.4836, Perplexity: 11.9843\n",
      "Epoch [0/2], Step [1330/3236], Loss: 2.5443, Perplexity: 12.7341\n",
      "Epoch [0/2], Step [1340/3236], Loss: 2.5614, Perplexity: 12.9541\n",
      "Epoch [0/2], Step [1350/3236], Loss: 2.8021, Perplexity: 16.4799\n",
      "Epoch [0/2], Step [1360/3236], Loss: 2.4483, Perplexity: 11.5684\n",
      "Epoch [0/2], Step [1370/3236], Loss: 2.5883, Perplexity: 13.3074\n",
      "Epoch [0/2], Step [1380/3236], Loss: 2.4541, Perplexity: 11.6357\n",
      "Epoch [0/2], Step [1390/3236], Loss: 2.5833, Perplexity: 13.2414\n",
      "Epoch [0/2], Step [1400/3236], Loss: 2.7277, Perplexity: 15.2978\n",
      "Epoch [0/2], Step [1410/3236], Loss: 2.7238, Perplexity: 15.2376\n",
      "Epoch [0/2], Step [1420/3236], Loss: 2.6060, Perplexity: 13.5446\n",
      "Epoch [0/2], Step [1430/3236], Loss: 2.5733, Perplexity: 13.1090\n",
      "Epoch [0/2], Step [1440/3236], Loss: 2.6839, Perplexity: 14.6418\n",
      "Epoch [0/2], Step [1450/3236], Loss: 2.5610, Perplexity: 12.9483\n",
      "Epoch [0/2], Step [1460/3236], Loss: 2.6123, Perplexity: 13.6310\n",
      "Epoch [0/2], Step [1470/3236], Loss: 2.5761, Perplexity: 13.1458\n",
      "Epoch [0/2], Step [1480/3236], Loss: 2.5743, Perplexity: 13.1227\n",
      "Epoch [0/2], Step [1490/3236], Loss: 2.7165, Perplexity: 15.1279\n",
      "Epoch [0/2], Step [1500/3236], Loss: 2.6841, Perplexity: 14.6454\n",
      "Epoch [0/2], Step [1510/3236], Loss: 2.5536, Perplexity: 12.8536\n",
      "Epoch [0/2], Step [1520/3236], Loss: 2.6884, Perplexity: 14.7079\n",
      "Epoch [0/2], Step [1530/3236], Loss: 2.7994, Perplexity: 16.4351\n",
      "Epoch [0/2], Step [1540/3236], Loss: 2.5977, Perplexity: 13.4322\n",
      "Epoch [0/2], Step [1550/3236], Loss: 2.5214, Perplexity: 12.4459\n",
      "Epoch [0/2], Step [1560/3236], Loss: 2.5637, Perplexity: 12.9838\n",
      "Epoch [0/2], Step [1570/3236], Loss: 2.7195, Perplexity: 15.1728\n",
      "Epoch [0/2], Step [1580/3236], Loss: 2.4759, Perplexity: 11.8924\n",
      "Epoch [0/2], Step [1590/3236], Loss: 2.4955, Perplexity: 12.1276\n",
      "Epoch [0/2], Step [1600/3236], Loss: 2.6612, Perplexity: 14.3136\n",
      "Epoch [0/2], Step [1610/3236], Loss: 2.4596, Perplexity: 11.7006\n",
      "Epoch [0/2], Step [1620/3236], Loss: 2.5994, Perplexity: 13.4560\n",
      "Epoch [0/2], Step [1630/3236], Loss: 2.4978, Perplexity: 12.1562\n",
      "Epoch [0/2], Step [1640/3236], Loss: 2.5364, Perplexity: 12.6342\n",
      "Epoch [0/2], Step [1650/3236], Loss: 2.5867, Perplexity: 13.2857\n",
      "Epoch [0/2], Step [1660/3236], Loss: 2.5193, Perplexity: 12.4198\n",
      "Epoch [0/2], Step [1670/3236], Loss: 2.6073, Perplexity: 13.5621\n",
      "Epoch [0/2], Step [1680/3236], Loss: 2.6648, Perplexity: 14.3657\n",
      "Epoch [0/2], Step [1690/3236], Loss: 2.4338, Perplexity: 11.4017\n",
      "Epoch [0/2], Step [1700/3236], Loss: 2.6101, Perplexity: 13.6003\n",
      "Epoch [0/2], Step [1710/3236], Loss: 2.5165, Perplexity: 12.3851\n",
      "Epoch [0/2], Step [1720/3236], Loss: 2.5878, Perplexity: 13.3011\n",
      "Epoch [0/2], Step [1730/3236], Loss: 2.5854, Perplexity: 13.2692\n",
      "Epoch [0/2], Step [1740/3236], Loss: 2.4003, Perplexity: 11.0267\n",
      "Epoch [0/2], Step [1750/3236], Loss: 2.6590, Perplexity: 14.2814\n",
      "Epoch [0/2], Step [1760/3236], Loss: 2.4188, Perplexity: 11.2320\n",
      "Epoch [0/2], Step [1770/3236], Loss: 2.5538, Perplexity: 12.8563\n",
      "Epoch [0/2], Step [1780/3236], Loss: 2.5927, Perplexity: 13.3660\n",
      "Epoch [0/2], Step [1790/3236], Loss: 2.4518, Perplexity: 11.6096\n",
      "Epoch [0/2], Step [1800/3236], Loss: 2.6595, Perplexity: 14.2888\n",
      "Epoch [0/2], Step [1810/3236], Loss: 2.5764, Perplexity: 13.1502\n",
      "Epoch [0/2], Step [1820/3236], Loss: 2.3789, Perplexity: 10.7933\n",
      "Epoch [0/2], Step [1830/3236], Loss: 2.4638, Perplexity: 11.7497\n",
      "Epoch [0/2], Step [1840/3236], Loss: 2.5811, Perplexity: 13.2118\n",
      "Epoch [0/2], Step [1850/3236], Loss: 2.5104, Perplexity: 12.3104\n",
      "Epoch [0/2], Step [1860/3236], Loss: 2.6169, Perplexity: 13.6936\n",
      "Epoch [0/2], Step [1870/3236], Loss: 2.4764, Perplexity: 11.8984\n",
      "Epoch [0/2], Step [1880/3236], Loss: 2.5712, Perplexity: 13.0814\n",
      "Epoch [0/2], Step [1890/3236], Loss: 2.5407, Perplexity: 12.6882\n",
      "Epoch [0/2], Step [1900/3236], Loss: 2.6558, Perplexity: 14.2365\n",
      "Epoch [0/2], Step [1910/3236], Loss: 2.4141, Perplexity: 11.1793\n",
      "Epoch [0/2], Step [1920/3236], Loss: 2.3930, Perplexity: 10.9464\n",
      "Epoch [0/2], Step [1930/3236], Loss: 2.4354, Perplexity: 11.4204\n",
      "Epoch [0/2], Step [1940/3236], Loss: 2.5280, Perplexity: 12.5282\n",
      "Epoch [0/2], Step [1950/3236], Loss: 2.4238, Perplexity: 11.2887\n",
      "Epoch [0/2], Step [1960/3236], Loss: 2.5097, Perplexity: 12.3008\n",
      "Epoch [0/2], Step [1970/3236], Loss: 2.4295, Perplexity: 11.3527\n",
      "Epoch [0/2], Step [1980/3236], Loss: 2.4186, Perplexity: 11.2301\n",
      "Epoch [0/2], Step [1990/3236], Loss: 2.4498, Perplexity: 11.5859\n",
      "Epoch [0/2], Step [2000/3236], Loss: 2.3664, Perplexity: 10.6593\n",
      "Epoch [0/2], Step [2010/3236], Loss: 2.5042, Perplexity: 12.2338\n",
      "Epoch [0/2], Step [2020/3236], Loss: 2.5968, Perplexity: 13.4209\n",
      "Epoch [0/2], Step [2030/3236], Loss: 2.4785, Perplexity: 11.9230\n",
      "Epoch [0/2], Step [2040/3236], Loss: 2.4582, Perplexity: 11.6842\n",
      "Epoch [0/2], Step [2050/3236], Loss: 2.4566, Perplexity: 11.6651\n",
      "Epoch [0/2], Step [2060/3236], Loss: 2.3981, Perplexity: 11.0017\n",
      "Epoch [0/2], Step [2070/3236], Loss: 2.4286, Perplexity: 11.3435\n",
      "Epoch [0/2], Step [2080/3236], Loss: 2.5128, Perplexity: 12.3388\n",
      "Epoch [0/2], Step [2090/3236], Loss: 2.6551, Perplexity: 14.2258\n",
      "Epoch [0/2], Step [2100/3236], Loss: 2.4672, Perplexity: 11.7893\n",
      "Epoch [0/2], Step [2110/3236], Loss: 2.4511, Perplexity: 11.6015\n",
      "Epoch [0/2], Step [2120/3236], Loss: 2.4324, Perplexity: 11.3865\n",
      "Epoch [0/2], Step [2130/3236], Loss: 2.4073, Perplexity: 11.1038\n",
      "Epoch [0/2], Step [2140/3236], Loss: 2.4069, Perplexity: 11.0998\n",
      "Epoch [0/2], Step [2150/3236], Loss: 2.4805, Perplexity: 11.9475\n",
      "Epoch [0/2], Step [2160/3236], Loss: 2.5345, Perplexity: 12.6097\n",
      "Epoch [0/2], Step [2170/3236], Loss: 2.5322, Perplexity: 12.5807\n",
      "Epoch [0/2], Step [2180/3236], Loss: 2.5054, Perplexity: 12.2482\n",
      "Epoch [0/2], Step [2190/3236], Loss: 2.6000, Perplexity: 13.4639\n",
      "Epoch [0/2], Step [2200/3236], Loss: 2.5085, Perplexity: 12.2870\n",
      "Epoch [0/2], Step [2210/3236], Loss: 2.4329, Perplexity: 11.3915\n",
      "Epoch [0/2], Step [2220/3236], Loss: 2.3261, Perplexity: 10.2375\n",
      "Epoch [0/2], Step [2230/3236], Loss: 2.5127, Perplexity: 12.3387\n",
      "Epoch [0/2], Step [2240/3236], Loss: 2.5166, Perplexity: 12.3860\n",
      "Epoch [0/2], Step [2250/3236], Loss: 2.4580, Perplexity: 11.6817\n",
      "Epoch [0/2], Step [2260/3236], Loss: 2.4424, Perplexity: 11.5009\n",
      "Epoch [0/2], Step [2270/3236], Loss: 2.2447, Perplexity: 9.4375\n",
      "Epoch [0/2], Step [2280/3236], Loss: 2.4899, Perplexity: 12.0599\n",
      "Epoch [0/2], Step [2290/3236], Loss: 2.4993, Perplexity: 12.1742\n",
      "Epoch [0/2], Step [2300/3236], Loss: 2.4009, Perplexity: 11.0327\n",
      "Epoch [0/2], Step [2310/3236], Loss: 2.3801, Perplexity: 10.8060\n",
      "Epoch [0/2], Step [2320/3236], Loss: 2.4349, Perplexity: 11.4144\n",
      "Epoch [0/2], Step [2330/3236], Loss: 2.3667, Perplexity: 10.6620\n",
      "Epoch [0/2], Step [2340/3236], Loss: 2.4781, Perplexity: 11.9181\n",
      "Epoch [0/2], Step [2350/3236], Loss: 2.3956, Perplexity: 10.9746\n",
      "Epoch [0/2], Step [2360/3236], Loss: 2.3683, Perplexity: 10.6794\n",
      "Epoch [0/2], Step [2370/3236], Loss: 2.3900, Perplexity: 10.9132\n",
      "Epoch [0/2], Step [2380/3236], Loss: 2.3957, Perplexity: 10.9758\n",
      "Epoch [0/2], Step [2390/3236], Loss: 2.4393, Perplexity: 11.4653\n",
      "Epoch [0/2], Step [2400/3236], Loss: 2.4525, Perplexity: 11.6169\n",
      "Epoch [0/2], Step [2410/3236], Loss: 2.3090, Perplexity: 10.0641\n",
      "Epoch [0/2], Step [2420/3236], Loss: 2.3016, Perplexity: 9.9898\n",
      "Epoch [0/2], Step [2430/3236], Loss: 2.4120, Perplexity: 11.1566\n",
      "Epoch [0/2], Step [2440/3236], Loss: 2.4136, Perplexity: 11.1737\n",
      "Epoch [0/2], Step [2450/3236], Loss: 2.2573, Perplexity: 9.5576\n",
      "Epoch [0/2], Step [2460/3236], Loss: 2.3597, Perplexity: 10.5874\n",
      "Epoch [0/2], Step [2470/3236], Loss: 2.5140, Perplexity: 12.3546\n",
      "Epoch [0/2], Step [2480/3236], Loss: 2.3214, Perplexity: 10.1896\n",
      "Epoch [0/2], Step [2490/3236], Loss: 2.3556, Perplexity: 10.5444\n",
      "Epoch [0/2], Step [2500/3236], Loss: 2.4736, Perplexity: 11.8653\n",
      "Epoch [0/2], Step [2510/3236], Loss: 2.4970, Perplexity: 12.1459\n",
      "Epoch [0/2], Step [2520/3236], Loss: 2.5703, Perplexity: 13.0700\n",
      "Epoch [0/2], Step [2530/3236], Loss: 2.3588, Perplexity: 10.5780\n",
      "Epoch [0/2], Step [2540/3236], Loss: 2.3883, Perplexity: 10.8950\n",
      "Epoch [0/2], Step [2550/3236], Loss: 2.4197, Perplexity: 11.2420\n",
      "Epoch [0/2], Step [2560/3236], Loss: 2.4420, Perplexity: 11.4963\n",
      "Epoch [0/2], Step [2570/3236], Loss: 2.3771, Perplexity: 10.7739\n",
      "Epoch [0/2], Step [2580/3236], Loss: 2.3410, Perplexity: 10.3920\n",
      "Epoch [0/2], Step [2590/3236], Loss: 2.2391, Perplexity: 9.3846\n",
      "Epoch [0/2], Step [2600/3236], Loss: 2.4667, Perplexity: 11.7838\n",
      "Epoch [0/2], Step [2610/3236], Loss: 2.2855, Perplexity: 9.8305\n",
      "Epoch [0/2], Step [2620/3236], Loss: 2.3310, Perplexity: 10.2886\n",
      "Epoch [0/2], Step [2630/3236], Loss: 2.3330, Perplexity: 10.3092\n",
      "Epoch [0/2], Step [2640/3236], Loss: 2.2911, Perplexity: 9.8860\n",
      "Epoch [0/2], Step [2650/3236], Loss: 2.3717, Perplexity: 10.7153\n",
      "Epoch [0/2], Step [2660/3236], Loss: 2.3923, Perplexity: 10.9391\n",
      "Epoch [0/2], Step [2670/3236], Loss: 2.4374, Perplexity: 11.4432\n",
      "Epoch [0/2], Step [2680/3236], Loss: 2.4895, Perplexity: 12.0555\n",
      "Epoch [0/2], Step [2690/3236], Loss: 2.3735, Perplexity: 10.7353\n",
      "Epoch [0/2], Step [2700/3236], Loss: 2.2366, Perplexity: 9.3613\n",
      "Epoch [0/2], Step [2710/3236], Loss: 2.3328, Perplexity: 10.3065\n",
      "Epoch [0/2], Step [2720/3236], Loss: 2.3177, Perplexity: 10.1518\n",
      "Epoch [0/2], Step [2730/3236], Loss: 2.5128, Perplexity: 12.3398\n",
      "Epoch [0/2], Step [2740/3236], Loss: 2.4385, Perplexity: 11.4558\n",
      "Epoch [0/2], Step [2750/3236], Loss: 2.3916, Perplexity: 10.9310\n",
      "Epoch [0/2], Step [2760/3236], Loss: 2.3735, Perplexity: 10.7345\n",
      "Epoch [0/2], Step [2770/3236], Loss: 2.3868, Perplexity: 10.8783\n",
      "Epoch [0/2], Step [2780/3236], Loss: 2.3595, Perplexity: 10.5854\n",
      "Epoch [0/2], Step [2790/3236], Loss: 2.4773, Perplexity: 11.9089\n",
      "Epoch [0/2], Step [2800/3236], Loss: 2.4867, Perplexity: 12.0218\n",
      "Epoch [0/2], Step [2810/3236], Loss: 2.3144, Perplexity: 10.1190\n",
      "Epoch [0/2], Step [2820/3236], Loss: 2.2199, Perplexity: 9.2068\n",
      "Epoch [0/2], Step [2830/3236], Loss: 2.4445, Perplexity: 11.5243\n",
      "Epoch [0/2], Step [2840/3236], Loss: 2.2465, Perplexity: 9.4547\n",
      "Epoch [0/2], Step [2850/3236], Loss: 2.2513, Perplexity: 9.4996\n",
      "Epoch [0/2], Step [2860/3236], Loss: 2.4139, Perplexity: 11.1772\n",
      "Epoch [0/2], Step [2870/3236], Loss: 2.4270, Perplexity: 11.3244\n",
      "Epoch [0/2], Step [2880/3236], Loss: 2.2336, Perplexity: 9.3333\n",
      "Epoch [0/2], Step [2890/3236], Loss: 2.4191, Perplexity: 11.2353\n",
      "Epoch [0/2], Step [2900/3236], Loss: 2.2341, Perplexity: 9.3378\n",
      "Epoch [0/2], Step [2910/3236], Loss: 2.3213, Perplexity: 10.1887\n",
      "Epoch [0/2], Step [2920/3236], Loss: 2.1942, Perplexity: 8.9729\n",
      "Epoch [0/2], Step [2930/3236], Loss: 2.3584, Perplexity: 10.5735\n",
      "Epoch [0/2], Step [2940/3236], Loss: 2.4360, Perplexity: 11.4275\n",
      "Epoch [0/2], Step [2950/3236], Loss: 2.2884, Perplexity: 9.8592\n",
      "Epoch [0/2], Step [2960/3236], Loss: 2.3917, Perplexity: 10.9325\n",
      "Epoch [0/2], Step [2970/3236], Loss: 2.2078, Perplexity: 9.0960\n",
      "Epoch [0/2], Step [2980/3236], Loss: 2.2501, Perplexity: 9.4888\n",
      "Epoch [0/2], Step [2990/3236], Loss: 2.3719, Perplexity: 10.7180\n",
      "Epoch [0/2], Step [3000/3236], Loss: 2.4675, Perplexity: 11.7926\n",
      "Epoch [0/2], Step [3010/3236], Loss: 2.4876, Perplexity: 12.0322\n",
      "Epoch [0/2], Step [3020/3236], Loss: 2.2541, Perplexity: 9.5267\n",
      "Epoch [0/2], Step [3030/3236], Loss: 2.4157, Perplexity: 11.1979\n",
      "Epoch [0/2], Step [3040/3236], Loss: 2.3480, Perplexity: 10.4650\n",
      "Epoch [0/2], Step [3050/3236], Loss: 2.2999, Perplexity: 9.9733\n",
      "Epoch [0/2], Step [3060/3236], Loss: 2.2606, Perplexity: 9.5892\n",
      "Epoch [0/2], Step [3070/3236], Loss: 2.4023, Perplexity: 11.0490\n",
      "Epoch [0/2], Step [3080/3236], Loss: 2.4071, Perplexity: 11.1018\n",
      "Epoch [0/2], Step [3090/3236], Loss: 2.2928, Perplexity: 9.9027\n",
      "Epoch [0/2], Step [3100/3236], Loss: 2.3199, Perplexity: 10.1745\n",
      "Epoch [0/2], Step [3110/3236], Loss: 2.4039, Perplexity: 11.0662\n",
      "Epoch [0/2], Step [3120/3236], Loss: 2.3648, Perplexity: 10.6422\n",
      "Epoch [0/2], Step [3130/3236], Loss: 2.3476, Perplexity: 10.4602\n",
      "Epoch [0/2], Step [3140/3236], Loss: 2.3303, Perplexity: 10.2813\n",
      "Epoch [0/2], Step [3150/3236], Loss: 2.2779, Perplexity: 9.7558\n",
      "Epoch [0/2], Step [3160/3236], Loss: 2.2773, Perplexity: 9.7508\n",
      "Epoch [0/2], Step [3170/3236], Loss: 2.4470, Perplexity: 11.5531\n",
      "Epoch [0/2], Step [3180/3236], Loss: 2.4475, Perplexity: 11.5595\n",
      "Epoch [0/2], Step [3190/3236], Loss: 2.4365, Perplexity: 11.4333\n",
      "Epoch [0/2], Step [3200/3236], Loss: 2.3964, Perplexity: 10.9839\n",
      "Epoch [0/2], Step [3210/3236], Loss: 2.2586, Perplexity: 9.5699\n",
      "Epoch [0/2], Step [3220/3236], Loss: 2.3250, Perplexity: 10.2271\n",
      "Epoch [0/2], Step [3230/3236], Loss: 2.2100, Perplexity: 9.1158\n",
      ".....>>> Begin epoch 1\n",
      "Epoch [1/2], Step [0/3236], Loss: 2.1315, Perplexity: 8.4273\n",
      "Epoch [1/2], Step [10/3236], Loss: 2.2726, Perplexity: 9.7051\n",
      "Epoch [1/2], Step [20/3236], Loss: 2.2941, Perplexity: 9.9154\n",
      "Epoch [1/2], Step [30/3236], Loss: 2.3402, Perplexity: 10.3837\n",
      "Epoch [1/2], Step [40/3236], Loss: 2.2225, Perplexity: 9.2302\n",
      "Epoch [1/2], Step [50/3236], Loss: 2.2539, Perplexity: 9.5252\n",
      "Epoch [1/2], Step [60/3236], Loss: 2.2729, Perplexity: 9.7072\n",
      "Epoch [1/2], Step [70/3236], Loss: 2.3528, Perplexity: 10.5147\n",
      "Epoch [1/2], Step [80/3236], Loss: 2.3229, Perplexity: 10.2050\n",
      "Epoch [1/2], Step [90/3236], Loss: 2.3329, Perplexity: 10.3075\n",
      "Epoch [1/2], Step [100/3236], Loss: 2.0557, Perplexity: 7.8120\n",
      "Epoch [1/2], Step [110/3236], Loss: 2.3025, Perplexity: 9.9989\n",
      "Epoch [1/2], Step [120/3236], Loss: 2.3209, Perplexity: 10.1845\n",
      "Epoch [1/2], Step [130/3236], Loss: 2.3788, Perplexity: 10.7918\n",
      "Epoch [1/2], Step [140/3236], Loss: 2.2849, Perplexity: 9.8242\n",
      "Epoch [1/2], Step [150/3236], Loss: 2.2890, Perplexity: 9.8652\n",
      "Epoch [1/2], Step [160/3236], Loss: 2.2354, Perplexity: 9.3504\n",
      "Epoch [1/2], Step [170/3236], Loss: 2.3336, Perplexity: 10.3152\n",
      "Epoch [1/2], Step [180/3236], Loss: 2.2304, Perplexity: 9.3036\n",
      "Epoch [1/2], Step [190/3236], Loss: 2.2212, Perplexity: 9.2187\n",
      "Epoch [1/2], Step [200/3236], Loss: 2.3454, Perplexity: 10.4377\n",
      "Epoch [1/2], Step [210/3236], Loss: 2.2902, Perplexity: 9.8765\n",
      "Epoch [1/2], Step [220/3236], Loss: 2.3150, Perplexity: 10.1247\n",
      "Epoch [1/2], Step [230/3236], Loss: 2.3224, Perplexity: 10.1999\n",
      "Epoch [1/2], Step [240/3236], Loss: 2.3738, Perplexity: 10.7379\n",
      "Epoch [1/2], Step [250/3236], Loss: 2.2706, Perplexity: 9.6856\n",
      "Epoch [1/2], Step [260/3236], Loss: 2.2879, Perplexity: 9.8546\n",
      "Epoch [1/2], Step [270/3236], Loss: 2.2756, Perplexity: 9.7335\n",
      "Epoch [1/2], Step [280/3236], Loss: 2.1332, Perplexity: 8.4417\n",
      "Epoch [1/2], Step [290/3236], Loss: 2.2005, Perplexity: 9.0298\n",
      "Epoch [1/2], Step [300/3236], Loss: 2.4209, Perplexity: 11.2559\n",
      "Epoch [1/2], Step [310/3236], Loss: 2.1823, Perplexity: 8.8669\n",
      "Epoch [1/2], Step [320/3236], Loss: 2.1748, Perplexity: 8.8005\n",
      "Epoch [1/2], Step [330/3236], Loss: 2.3377, Perplexity: 10.3573\n",
      "Epoch [1/2], Step [340/3236], Loss: 2.1811, Perplexity: 8.8559\n",
      "Epoch [1/2], Step [350/3236], Loss: 2.2357, Perplexity: 9.3526\n",
      "Epoch [1/2], Step [360/3236], Loss: 2.1861, Perplexity: 8.9005\n",
      "Epoch [1/2], Step [370/3236], Loss: 2.2688, Perplexity: 9.6674\n",
      "Epoch [1/2], Step [380/3236], Loss: 2.3077, Perplexity: 10.0510\n",
      "Epoch [1/2], Step [390/3236], Loss: 2.1668, Perplexity: 8.7303\n",
      "Epoch [1/2], Step [400/3236], Loss: 2.2630, Perplexity: 9.6121\n",
      "Epoch [1/2], Step [410/3236], Loss: 2.1928, Perplexity: 8.9605\n",
      "Epoch [1/2], Step [420/3236], Loss: 2.2754, Perplexity: 9.7314\n",
      "Epoch [1/2], Step [430/3236], Loss: 2.2456, Perplexity: 9.4457\n",
      "Epoch [1/2], Step [440/3236], Loss: 2.2161, Perplexity: 9.1710\n",
      "Epoch [1/2], Step [450/3236], Loss: 2.1842, Perplexity: 8.8835\n",
      "Epoch [1/2], Step [460/3236], Loss: 2.2464, Perplexity: 9.4536\n",
      "Epoch [1/2], Step [470/3236], Loss: 2.3273, Perplexity: 10.2498\n",
      "Epoch [1/2], Step [480/3236], Loss: 2.3007, Perplexity: 9.9809\n",
      "Epoch [1/2], Step [490/3236], Loss: 2.1382, Perplexity: 8.4838\n",
      "Epoch [1/2], Step [500/3236], Loss: 2.2021, Perplexity: 9.0441\n",
      "Epoch [1/2], Step [510/3236], Loss: 2.1205, Perplexity: 8.3352\n",
      "Epoch [1/2], Step [520/3236], Loss: 2.2672, Perplexity: 9.6520\n",
      "Epoch [1/2], Step [530/3236], Loss: 2.2853, Perplexity: 9.8289\n",
      "Epoch [1/2], Step [540/3236], Loss: 2.2573, Perplexity: 9.5573\n",
      "Epoch [1/2], Step [550/3236], Loss: 2.2680, Perplexity: 9.6596\n",
      "Epoch [1/2], Step [560/3236], Loss: 2.3395, Perplexity: 10.3756\n",
      "Epoch [1/2], Step [570/3236], Loss: 2.0840, Perplexity: 8.0367\n",
      "Epoch [1/2], Step [580/3236], Loss: 2.1891, Perplexity: 8.9272\n",
      "Epoch [1/2], Step [590/3236], Loss: 2.2868, Perplexity: 9.8437\n",
      "Epoch [1/2], Step [600/3236], Loss: 2.2939, Perplexity: 9.9134\n",
      "Epoch [1/2], Step [610/3236], Loss: 2.2684, Perplexity: 9.6642\n",
      "Epoch [1/2], Step [620/3236], Loss: 2.1372, Perplexity: 8.4753\n",
      "Epoch [1/2], Step [630/3236], Loss: 2.0786, Perplexity: 7.9933\n",
      "Epoch [1/2], Step [640/3236], Loss: 2.2704, Perplexity: 9.6837\n",
      "Epoch [1/2], Step [650/3236], Loss: 2.2121, Perplexity: 9.1345\n",
      "Epoch [1/2], Step [660/3236], Loss: 2.1991, Perplexity: 9.0170\n",
      "Epoch [1/2], Step [670/3236], Loss: 2.2722, Perplexity: 9.7005\n",
      "Epoch [1/2], Step [680/3236], Loss: 2.2880, Perplexity: 9.8549\n",
      "Epoch [1/2], Step [690/3236], Loss: 2.2556, Perplexity: 9.5410\n",
      "Epoch [1/2], Step [700/3236], Loss: 2.1769, Perplexity: 8.8190\n",
      "Epoch [1/2], Step [710/3236], Loss: 2.1949, Perplexity: 8.9793\n",
      "Epoch [1/2], Step [720/3236], Loss: 2.1589, Perplexity: 8.6612\n",
      "Epoch [1/2], Step [730/3236], Loss: 2.3263, Perplexity: 10.2401\n",
      "Epoch [1/2], Step [740/3236], Loss: 2.1674, Perplexity: 8.7359\n",
      "Epoch [1/2], Step [750/3236], Loss: 2.0736, Perplexity: 7.9532\n",
      "Epoch [1/2], Step [760/3236], Loss: 2.2011, Perplexity: 9.0345\n",
      "Epoch [1/2], Step [770/3236], Loss: 2.2114, Perplexity: 9.1287\n",
      "Epoch [1/2], Step [780/3236], Loss: 2.2780, Perplexity: 9.7569\n",
      "Epoch [1/2], Step [790/3236], Loss: 2.3833, Perplexity: 10.8401\n",
      "Epoch [1/2], Step [800/3236], Loss: 2.2623, Perplexity: 9.6054\n",
      "Epoch [1/2], Step [810/3236], Loss: 2.1507, Perplexity: 8.5910\n",
      "Epoch [1/2], Step [820/3236], Loss: 2.1516, Perplexity: 8.5988\n",
      "Epoch [1/2], Step [830/3236], Loss: 2.2802, Perplexity: 9.7786\n",
      "Epoch [1/2], Step [840/3236], Loss: 2.1733, Perplexity: 8.7869\n",
      "Epoch [1/2], Step [850/3236], Loss: 2.0710, Perplexity: 7.9326\n",
      "Epoch [1/2], Step [860/3236], Loss: 2.2820, Perplexity: 9.7964\n",
      "Epoch [1/2], Step [870/3236], Loss: 2.2213, Perplexity: 9.2191\n",
      "Epoch [1/2], Step [880/3236], Loss: 2.0818, Perplexity: 8.0193\n",
      "Epoch [1/2], Step [890/3236], Loss: 2.4578, Perplexity: 11.6795\n",
      "Epoch [1/2], Step [900/3236], Loss: 2.2298, Perplexity: 9.2984\n",
      "Epoch [1/2], Step [910/3236], Loss: 2.0268, Perplexity: 7.5896\n",
      "Epoch [1/2], Step [920/3236], Loss: 2.3008, Perplexity: 9.9823\n",
      "Epoch [1/2], Step [930/3236], Loss: 2.2097, Perplexity: 9.1129\n",
      "Epoch [1/2], Step [940/3236], Loss: 2.2661, Perplexity: 9.6421\n",
      "Epoch [1/2], Step [950/3236], Loss: 2.3157, Perplexity: 10.1324\n",
      "Epoch [1/2], Step [960/3236], Loss: 2.0953, Perplexity: 8.1280\n",
      "Epoch [1/2], Step [970/3236], Loss: 2.0839, Perplexity: 8.0357\n",
      "Epoch [1/2], Step [980/3236], Loss: 2.1547, Perplexity: 8.6250\n",
      "Epoch [1/2], Step [990/3236], Loss: 2.2875, Perplexity: 9.8502\n",
      "Epoch [1/2], Step [1000/3236], Loss: 2.2560, Perplexity: 9.5446\n",
      "Epoch [1/2], Step [1010/3236], Loss: 2.2379, Perplexity: 9.3740\n",
      "Epoch [1/2], Step [1020/3236], Loss: 2.2132, Perplexity: 9.1447\n",
      "Epoch [1/2], Step [1030/3236], Loss: 2.2367, Perplexity: 9.3621\n",
      "Epoch [1/2], Step [1040/3236], Loss: 2.1644, Perplexity: 8.7098\n",
      "Epoch [1/2], Step [1050/3236], Loss: 2.3042, Perplexity: 10.0166\n",
      "Epoch [1/2], Step [1060/3236], Loss: 2.0798, Perplexity: 8.0025\n",
      "Epoch [1/2], Step [1070/3236], Loss: 2.2149, Perplexity: 9.1608\n",
      "Epoch [1/2], Step [1080/3236], Loss: 2.2408, Perplexity: 9.4009\n",
      "Epoch [1/2], Step [1090/3236], Loss: 2.3454, Perplexity: 10.4378\n",
      "Epoch [1/2], Step [1100/3236], Loss: 2.4264, Perplexity: 11.3185\n",
      "Epoch [1/2], Step [1110/3236], Loss: 2.2114, Perplexity: 9.1280\n",
      "Epoch [1/2], Step [1120/3236], Loss: 2.2874, Perplexity: 9.8491\n",
      "Epoch [1/2], Step [1130/3236], Loss: 2.1298, Perplexity: 8.4130\n",
      "Epoch [1/2], Step [1140/3236], Loss: 2.2918, Perplexity: 9.8926\n",
      "Epoch [1/2], Step [1150/3236], Loss: 2.1720, Perplexity: 8.7761\n",
      "Epoch [1/2], Step [1160/3236], Loss: 2.2664, Perplexity: 9.6447\n",
      "Epoch [1/2], Step [1170/3236], Loss: 2.3459, Perplexity: 10.4426\n",
      "Epoch [1/2], Step [1180/3236], Loss: 2.1474, Perplexity: 8.5629\n",
      "Epoch [1/2], Step [1190/3236], Loss: 2.0891, Perplexity: 8.0776\n",
      "Epoch [1/2], Step [1200/3236], Loss: 2.1158, Perplexity: 8.2960\n",
      "Epoch [1/2], Step [1210/3236], Loss: 2.1044, Perplexity: 8.2024\n",
      "Epoch [1/2], Step [1220/3236], Loss: 2.2376, Perplexity: 9.3711\n",
      "Epoch [1/2], Step [1230/3236], Loss: 2.1649, Perplexity: 8.7138\n",
      "Epoch [1/2], Step [1240/3236], Loss: 2.2717, Perplexity: 9.6961\n",
      "Epoch [1/2], Step [1250/3236], Loss: 2.2183, Perplexity: 9.1918\n",
      "Epoch [1/2], Step [1260/3236], Loss: 2.2824, Perplexity: 9.8005\n",
      "Epoch [1/2], Step [1270/3236], Loss: 2.1544, Perplexity: 8.6231\n",
      "Epoch [1/2], Step [1280/3236], Loss: 2.2132, Perplexity: 9.1445\n",
      "Epoch [1/2], Step [1290/3236], Loss: 2.1913, Perplexity: 8.9470\n",
      "Epoch [1/2], Step [1300/3236], Loss: 2.2702, Perplexity: 9.6814\n",
      "Epoch [1/2], Step [1310/3236], Loss: 2.0952, Perplexity: 8.1270\n",
      "Epoch [1/2], Step [1320/3236], Loss: 2.1621, Perplexity: 8.6895\n",
      "Epoch [1/2], Step [1330/3236], Loss: 2.1898, Perplexity: 8.9332\n",
      "Epoch [1/2], Step [1340/3236], Loss: 2.2245, Perplexity: 9.2487\n",
      "Epoch [1/2], Step [1350/3236], Loss: 2.0839, Perplexity: 8.0359\n",
      "Epoch [1/2], Step [1360/3236], Loss: 2.1228, Perplexity: 8.3545\n",
      "Epoch [1/2], Step [1370/3236], Loss: 2.1340, Perplexity: 8.4484\n",
      "Epoch [1/2], Step [1380/3236], Loss: 2.1550, Perplexity: 8.6275\n",
      "Epoch [1/2], Step [1390/3236], Loss: 2.2664, Perplexity: 9.6444\n",
      "Epoch [1/2], Step [1400/3236], Loss: 2.1384, Perplexity: 8.4862\n",
      "Epoch [1/2], Step [1410/3236], Loss: 2.1693, Perplexity: 8.7518\n",
      "Epoch [1/2], Step [1420/3236], Loss: 2.3384, Perplexity: 10.3644\n",
      "Epoch [1/2], Step [1430/3236], Loss: 2.2870, Perplexity: 9.8452\n",
      "Epoch [1/2], Step [1440/3236], Loss: 2.1548, Perplexity: 8.6261\n",
      "Epoch [1/2], Step [1450/3236], Loss: 2.1460, Perplexity: 8.5510\n",
      "Epoch [1/2], Step [1460/3236], Loss: 2.1682, Perplexity: 8.7423\n",
      "Epoch [1/2], Step [1470/3236], Loss: 2.2345, Perplexity: 9.3416\n",
      "Epoch [1/2], Step [1480/3236], Loss: 2.1501, Perplexity: 8.5855\n",
      "Epoch [1/2], Step [1490/3236], Loss: 2.2698, Perplexity: 9.6772\n",
      "Epoch [1/2], Step [1500/3236], Loss: 2.2852, Perplexity: 9.8272\n",
      "Epoch [1/2], Step [1510/3236], Loss: 2.2480, Perplexity: 9.4691\n",
      "Epoch [1/2], Step [1520/3236], Loss: 2.3538, Perplexity: 10.5254\n",
      "Epoch [1/2], Step [1530/3236], Loss: 2.0796, Perplexity: 8.0013\n",
      "Epoch [1/2], Step [1540/3236], Loss: 2.3034, Perplexity: 10.0080\n",
      "Epoch [1/2], Step [1550/3236], Loss: 2.2800, Perplexity: 9.7770\n",
      "Epoch [1/2], Step [1560/3236], Loss: 2.1724, Perplexity: 8.7795\n",
      "Epoch [1/2], Step [1570/3236], Loss: 2.2812, Perplexity: 9.7888\n",
      "Epoch [1/2], Step [1580/3236], Loss: 2.1081, Perplexity: 8.2329\n",
      "Epoch [1/2], Step [1590/3236], Loss: 2.1689, Perplexity: 8.7490\n",
      "Epoch [1/2], Step [1600/3236], Loss: 2.1085, Perplexity: 8.2357\n",
      "Epoch [1/2], Step [1610/3236], Loss: 2.1042, Perplexity: 8.2004\n",
      "Epoch [1/2], Step [1620/3236], Loss: 2.2579, Perplexity: 9.5629\n",
      "Epoch [1/2], Step [1630/3236], Loss: 2.2409, Perplexity: 9.4014\n",
      "Epoch [1/2], Step [1640/3236], Loss: 2.1629, Perplexity: 8.6966\n",
      "Epoch [1/2], Step [1650/3236], Loss: 2.1500, Perplexity: 8.5850\n",
      "Epoch [1/2], Step [1660/3236], Loss: 2.1948, Perplexity: 8.9779\n",
      "Epoch [1/2], Step [1670/3236], Loss: 2.1438, Perplexity: 8.5320\n",
      "Epoch [1/2], Step [1680/3236], Loss: 2.1855, Perplexity: 8.8953\n",
      "Epoch [1/2], Step [1690/3236], Loss: 2.1842, Perplexity: 8.8837\n",
      "Epoch [1/2], Step [1700/3236], Loss: 2.1424, Perplexity: 8.5202\n",
      "Epoch [1/2], Step [1710/3236], Loss: 2.1179, Perplexity: 8.3135\n",
      "Epoch [1/2], Step [1720/3236], Loss: 2.1329, Perplexity: 8.4391\n",
      "Epoch [1/2], Step [1730/3236], Loss: 2.2496, Perplexity: 9.4840\n",
      "Epoch [1/2], Step [1740/3236], Loss: 2.1488, Perplexity: 8.5748\n",
      "Epoch [1/2], Step [1750/3236], Loss: 2.1767, Perplexity: 8.8174\n",
      "Epoch [1/2], Step [1760/3236], Loss: 2.2168, Perplexity: 9.1776\n",
      "Epoch [1/2], Step [1770/3236], Loss: 2.1786, Perplexity: 8.8340\n",
      "Epoch [1/2], Step [1780/3236], Loss: 2.2300, Perplexity: 9.3001\n",
      "Epoch [1/2], Step [1790/3236], Loss: 2.1962, Perplexity: 8.9912\n",
      "Epoch [1/2], Step [1800/3236], Loss: 2.1983, Perplexity: 9.0097\n",
      "Epoch [1/2], Step [1810/3236], Loss: 2.1751, Perplexity: 8.8032\n",
      "Epoch [1/2], Step [1820/3236], Loss: 2.1823, Perplexity: 8.8670\n",
      "Epoch [1/2], Step [1830/3236], Loss: 2.1538, Perplexity: 8.6176\n",
      "Epoch [1/2], Step [1840/3236], Loss: 2.2528, Perplexity: 9.5139\n",
      "Epoch [1/2], Step [1850/3236], Loss: 2.1672, Perplexity: 8.7335\n",
      "Epoch [1/2], Step [1860/3236], Loss: 2.3336, Perplexity: 10.3149\n",
      "Epoch [1/2], Step [1870/3236], Loss: 2.0209, Perplexity: 7.5454\n",
      "Epoch [1/2], Step [1880/3236], Loss: 2.3331, Perplexity: 10.3103\n",
      "Epoch [1/2], Step [1890/3236], Loss: 2.1868, Perplexity: 8.9071\n",
      "Epoch [1/2], Step [1900/3236], Loss: 2.1639, Perplexity: 8.7051\n",
      "Epoch [1/2], Step [1910/3236], Loss: 2.2245, Perplexity: 9.2486\n",
      "Epoch [1/2], Step [1920/3236], Loss: 2.1392, Perplexity: 8.4924\n",
      "Epoch [1/2], Step [1930/3236], Loss: 2.2358, Perplexity: 9.3542\n",
      "Epoch [1/2], Step [1940/3236], Loss: 2.0623, Perplexity: 7.8637\n",
      "Epoch [1/2], Step [1950/3236], Loss: 2.0742, Perplexity: 7.9585\n",
      "Epoch [1/2], Step [1960/3236], Loss: 2.2160, Perplexity: 9.1707\n",
      "Epoch [1/2], Step [1970/3236], Loss: 2.2102, Perplexity: 9.1171\n",
      "Epoch [1/2], Step [1980/3236], Loss: 2.1173, Perplexity: 8.3085\n",
      "Epoch [1/2], Step [1990/3236], Loss: 2.0988, Perplexity: 8.1564\n",
      "Epoch [1/2], Step [2000/3236], Loss: 2.3079, Perplexity: 10.0536\n",
      "Epoch [1/2], Step [2010/3236], Loss: 2.1728, Perplexity: 8.7831\n",
      "Epoch [1/2], Step [2020/3236], Loss: 2.2773, Perplexity: 9.7502\n",
      "Epoch [1/2], Step [2030/3236], Loss: 2.1869, Perplexity: 8.9077\n",
      "Epoch [1/2], Step [2040/3236], Loss: 2.1636, Perplexity: 8.7027\n",
      "Epoch [1/2], Step [2050/3236], Loss: 2.1710, Perplexity: 8.7669\n",
      "Epoch [1/2], Step [2060/3236], Loss: 2.1490, Perplexity: 8.5766\n",
      "Epoch [1/2], Step [2070/3236], Loss: 2.2955, Perplexity: 9.9291\n",
      "Epoch [1/2], Step [2080/3236], Loss: 2.1635, Perplexity: 8.7016\n",
      "Epoch [1/2], Step [2090/3236], Loss: 2.2393, Perplexity: 9.3871\n",
      "Epoch [1/2], Step [2100/3236], Loss: 2.0236, Perplexity: 7.5651\n",
      "Epoch [1/2], Step [2110/3236], Loss: 2.1327, Perplexity: 8.4379\n",
      "Epoch [1/2], Step [2120/3236], Loss: 2.2300, Perplexity: 9.2995\n",
      "Epoch [1/2], Step [2130/3236], Loss: 2.2109, Perplexity: 9.1235\n",
      "Epoch [1/2], Step [2140/3236], Loss: 2.0997, Perplexity: 8.1640\n",
      "Epoch [1/2], Step [2150/3236], Loss: 2.1399, Perplexity: 8.4989\n",
      "Epoch [1/2], Step [2160/3236], Loss: 2.2476, Perplexity: 9.4648\n",
      "Epoch [1/2], Step [2170/3236], Loss: 2.2651, Perplexity: 9.6318\n",
      "Epoch [1/2], Step [2180/3236], Loss: 2.1629, Perplexity: 8.6964\n",
      "Epoch [1/2], Step [2190/3236], Loss: 2.1922, Perplexity: 8.9545\n",
      "Epoch [1/2], Step [2200/3236], Loss: 2.2650, Perplexity: 9.6315\n",
      "Epoch [1/2], Step [2210/3236], Loss: 2.1387, Perplexity: 8.4884\n",
      "Epoch [1/2], Step [2220/3236], Loss: 2.1199, Perplexity: 8.3306\n",
      "Epoch [1/2], Step [2230/3236], Loss: 2.1291, Perplexity: 8.4075\n",
      "Epoch [1/2], Step [2240/3236], Loss: 2.0733, Perplexity: 7.9510\n",
      "Epoch [1/2], Step [2250/3236], Loss: 2.1412, Perplexity: 8.5097\n",
      "Epoch [1/2], Step [2260/3236], Loss: 2.2760, Perplexity: 9.7376\n",
      "Epoch [1/2], Step [2270/3236], Loss: 2.1411, Perplexity: 8.5091\n",
      "Epoch [1/2], Step [2280/3236], Loss: 2.2663, Perplexity: 9.6438\n",
      "Epoch [1/2], Step [2290/3236], Loss: 2.1181, Perplexity: 8.3155\n",
      "Epoch [1/2], Step [2300/3236], Loss: 2.2179, Perplexity: 9.1877\n",
      "Epoch [1/2], Step [2310/3236], Loss: 2.1440, Perplexity: 8.5333\n",
      "Epoch [1/2], Step [2320/3236], Loss: 2.0082, Perplexity: 7.4499\n",
      "Epoch [1/2], Step [2330/3236], Loss: 2.1365, Perplexity: 8.4694\n",
      "Epoch [1/2], Step [2340/3236], Loss: 2.1312, Perplexity: 8.4252\n",
      "Epoch [1/2], Step [2350/3236], Loss: 2.0827, Perplexity: 8.0259\n",
      "Epoch [1/2], Step [2360/3236], Loss: 2.2001, Perplexity: 9.0257\n",
      "Epoch [1/2], Step [2370/3236], Loss: 2.1401, Perplexity: 8.5007\n",
      "Epoch [1/2], Step [2380/3236], Loss: 2.1304, Perplexity: 8.4182\n",
      "Epoch [1/2], Step [2390/3236], Loss: 2.1533, Perplexity: 8.6136\n",
      "Epoch [1/2], Step [2400/3236], Loss: 2.1810, Perplexity: 8.8552\n",
      "Epoch [1/2], Step [2410/3236], Loss: 2.2665, Perplexity: 9.6454\n",
      "Epoch [1/2], Step [2420/3236], Loss: 2.0969, Perplexity: 8.1410\n",
      "Epoch [1/2], Step [2430/3236], Loss: 2.1782, Perplexity: 8.8307\n",
      "Epoch [1/2], Step [2440/3236], Loss: 2.1707, Perplexity: 8.7644\n",
      "Epoch [1/2], Step [2450/3236], Loss: 2.1296, Perplexity: 8.4113\n",
      "Epoch [1/2], Step [2460/3236], Loss: 2.0338, Perplexity: 7.6430\n",
      "Epoch [1/2], Step [2470/3236], Loss: 2.0617, Perplexity: 7.8590\n",
      "Epoch [1/2], Step [2480/3236], Loss: 2.2058, Perplexity: 9.0774\n",
      "Epoch [1/2], Step [2490/3236], Loss: 2.2566, Perplexity: 9.5506\n",
      "Epoch [1/2], Step [2500/3236], Loss: 2.0839, Perplexity: 8.0354\n",
      "Epoch [1/2], Step [2510/3236], Loss: 2.1759, Perplexity: 8.8102\n",
      "Epoch [1/2], Step [2520/3236], Loss: 2.1358, Perplexity: 8.4638\n",
      "Epoch [1/2], Step [2530/3236], Loss: 2.1408, Perplexity: 8.5059\n",
      "Epoch [1/2], Step [2540/3236], Loss: 2.3397, Perplexity: 10.3780\n",
      "Epoch [1/2], Step [2550/3236], Loss: 2.2149, Perplexity: 9.1601\n",
      "Epoch [1/2], Step [2560/3236], Loss: 2.1559, Perplexity: 8.6354\n",
      "Epoch [1/2], Step [2570/3236], Loss: 2.0507, Perplexity: 7.7733\n",
      "Epoch [1/2], Step [2580/3236], Loss: 2.1913, Perplexity: 8.9466\n",
      "Epoch [1/2], Step [2590/3236], Loss: 2.1662, Perplexity: 8.7248\n",
      "Epoch [1/2], Step [2600/3236], Loss: 2.0857, Perplexity: 8.0504\n",
      "Epoch [1/2], Step [2610/3236], Loss: 2.0436, Perplexity: 7.7184\n",
      "Epoch [1/2], Step [2620/3236], Loss: 2.0207, Perplexity: 7.5433\n",
      "Epoch [1/2], Step [2630/3236], Loss: 2.1813, Perplexity: 8.8578\n",
      "Epoch [1/2], Step [2640/3236], Loss: 2.1859, Perplexity: 8.8986\n",
      "Epoch [1/2], Step [2650/3236], Loss: 2.1196, Perplexity: 8.3280\n",
      "Epoch [1/2], Step [2660/3236], Loss: 2.0815, Perplexity: 8.0166\n",
      "Epoch [1/2], Step [2670/3236], Loss: 2.0048, Perplexity: 7.4248\n",
      "Epoch [1/2], Step [2680/3236], Loss: 2.1469, Perplexity: 8.5586\n",
      "Epoch [1/2], Step [2690/3236], Loss: 2.0297, Perplexity: 7.6118\n",
      "Epoch [1/2], Step [2700/3236], Loss: 2.0537, Perplexity: 7.7966\n",
      "Epoch [1/2], Step [2710/3236], Loss: 2.1381, Perplexity: 8.4831\n",
      "Epoch [1/2], Step [2720/3236], Loss: 2.0290, Perplexity: 7.6067\n",
      "Epoch [1/2], Step [2730/3236], Loss: 2.2083, Perplexity: 9.1003\n",
      "Epoch [1/2], Step [2740/3236], Loss: 2.1384, Perplexity: 8.4855\n",
      "Epoch [1/2], Step [2750/3236], Loss: 2.2268, Perplexity: 9.2706\n",
      "Epoch [1/2], Step [2760/3236], Loss: 2.2034, Perplexity: 9.0558\n",
      "Epoch [1/2], Step [2770/3236], Loss: 2.2290, Perplexity: 9.2901\n",
      "Epoch [1/2], Step [2780/3236], Loss: 2.1662, Perplexity: 8.7249\n",
      "Epoch [1/2], Step [2790/3236], Loss: 2.0971, Perplexity: 8.1429\n",
      "Epoch [1/2], Step [2800/3236], Loss: 2.4110, Perplexity: 11.1450\n",
      "Epoch [1/2], Step [2810/3236], Loss: 2.1022, Perplexity: 8.1843\n",
      "Epoch [1/2], Step [2820/3236], Loss: 2.0950, Perplexity: 8.1251\n",
      "Epoch [1/2], Step [2830/3236], Loss: 2.1364, Perplexity: 8.4688\n",
      "Epoch [1/2], Step [2840/3236], Loss: 2.2147, Perplexity: 9.1585\n",
      "Epoch [1/2], Step [2850/3236], Loss: 2.0945, Perplexity: 8.1216\n",
      "Epoch [1/2], Step [2860/3236], Loss: 2.1430, Perplexity: 8.5250\n",
      "Epoch [1/2], Step [2870/3236], Loss: 2.1159, Perplexity: 8.2969\n",
      "Epoch [1/2], Step [2880/3236], Loss: 2.2235, Perplexity: 9.2400\n",
      "Epoch [1/2], Step [2890/3236], Loss: 2.0981, Perplexity: 8.1510\n",
      "Epoch [1/2], Step [2900/3236], Loss: 2.1367, Perplexity: 8.4713\n",
      "Epoch [1/2], Step [2910/3236], Loss: 2.1950, Perplexity: 8.9796\n",
      "Epoch [1/2], Step [2920/3236], Loss: 2.1156, Perplexity: 8.2943\n",
      "Epoch [1/2], Step [2930/3236], Loss: 2.1681, Perplexity: 8.7413\n",
      "Epoch [1/2], Step [2940/3236], Loss: 2.1821, Perplexity: 8.8646\n",
      "Epoch [1/2], Step [2950/3236], Loss: 2.0810, Perplexity: 8.0127\n",
      "Epoch [1/2], Step [2960/3236], Loss: 2.2708, Perplexity: 9.6875\n",
      "Epoch [1/2], Step [2970/3236], Loss: 2.0430, Perplexity: 7.7140\n",
      "Epoch [1/2], Step [2980/3236], Loss: 2.1353, Perplexity: 8.4598\n",
      "Epoch [1/2], Step [2990/3236], Loss: 2.1536, Perplexity: 8.6158\n",
      "Epoch [1/2], Step [3000/3236], Loss: 2.2546, Perplexity: 9.5311\n",
      "Epoch [1/2], Step [3010/3236], Loss: 2.1916, Perplexity: 8.9496\n",
      "Epoch [1/2], Step [3020/3236], Loss: 2.3229, Perplexity: 10.2056\n",
      "Epoch [1/2], Step [3030/3236], Loss: 2.1934, Perplexity: 8.9658\n",
      "Epoch [1/2], Step [3040/3236], Loss: 2.0537, Perplexity: 7.7967\n",
      "Epoch [1/2], Step [3050/3236], Loss: 2.1798, Perplexity: 8.8448\n",
      "Epoch [1/2], Step [3060/3236], Loss: 2.1453, Perplexity: 8.5446\n",
      "Epoch [1/2], Step [3070/3236], Loss: 2.0628, Perplexity: 7.8682\n",
      "Epoch [1/2], Step [3080/3236], Loss: 2.1909, Perplexity: 8.9435\n",
      "Epoch [1/2], Step [3090/3236], Loss: 2.0864, Perplexity: 8.0560\n",
      "Epoch [1/2], Step [3100/3236], Loss: 2.1088, Perplexity: 8.2385\n",
      "Epoch [1/2], Step [3110/3236], Loss: 2.0675, Perplexity: 7.9048\n",
      "Epoch [1/2], Step [3120/3236], Loss: 2.1526, Perplexity: 8.6071\n",
      "Epoch [1/2], Step [3130/3236], Loss: 2.0191, Perplexity: 7.5315\n",
      "Epoch [1/2], Step [3140/3236], Loss: 2.1708, Perplexity: 8.7650\n",
      "Epoch [1/2], Step [3150/3236], Loss: 2.1377, Perplexity: 8.4800\n",
      "Epoch [1/2], Step [3160/3236], Loss: 2.2420, Perplexity: 9.4118\n",
      "Epoch [1/2], Step [3170/3236], Loss: 2.1416, Perplexity: 8.5134\n",
      "Epoch [1/2], Step [3180/3236], Loss: 1.9135, Perplexity: 6.7769\n",
      "Epoch [1/2], Step [3190/3236], Loss: 2.2006, Perplexity: 9.0304\n",
      "Epoch [1/2], Step [3200/3236], Loss: 2.0833, Perplexity: 8.0311\n",
      "Epoch [1/2], Step [3210/3236], Loss: 2.1359, Perplexity: 8.4646\n",
      "Epoch [1/2], Step [3220/3236], Loss: 2.1414, Perplexity: 8.5112\n",
      "Epoch [1/2], Step [3230/3236], Loss: 2.1338, Perplexity: 8.4473\n",
      "....."
     ]
    }
   ],
   "source": [
    "print(args, '\\n')\n",
    "\n",
    "# Train the models\n",
    "total_step = len(data_loader)\n",
    "for epoch in range(args.num_epochs):\n",
    "    print('>>> Begin epoch {}'.format(epoch))\n",
    "\n",
    "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "        # Set mini-batch dataset\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print log info\n",
    "        if i % args.log_step == 0:\n",
    "            print('\\rEpoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                  .format(epoch, args.num_epochs, i, total_step, loss.item(), np.exp(loss.item())))\n",
    "        else:\n",
    "            print('.', end='')\n",
    "\n",
    "        # Save the model checkpoints\n",
    "        if (i+1) % args.save_step == 0:\n",
    "            torch.save(decoder.state_dict(), os.path.join(\n",
    "                args.model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "            torch.save(encoder.state_dict(), os.path.join(\n",
    "                args.model_path, 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9bf2c-259f-4eaa-b92e-75afcbfc1ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-py39-tf",
   "language": "python",
   "name": "conda-py39-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
