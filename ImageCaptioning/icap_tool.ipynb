{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7e5e37-aaf0-42df-9fa4-978cad19a6a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ICAP_TOOL\n",
    "\n",
    "The ```icap_tool``` is a command line utility written in Python that allows you to flexibly specify the dataset of desired scales, different types of models and desired number of epochs.\n",
    "\n",
    "Icap_tool is composed of three main subcommands: train, predict and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9443fdc7-2805-4a73-b48d-3b5e86f82b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: icap_tool.py [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  evaluate\n",
      "  predict\n",
      "  train\n"
     ]
    }
   ],
   "source": [
    "!python icap_tool.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a48e48-4352-4dde-80c7-b12f9a613eda",
   "metadata": {},
   "source": [
    "## 'Train' subcommand\n",
    "\n",
    "1) Dataset selection<br/>\n",
    "By default, the command will use full 30k images of Flickr30k dataset and split it into 80% train set and 20% test set.<br/>\n",
    "Howevever, you can use part of the data for a quick run. For example, \"-d Flickr30k_8000_0.8\" option will use only 8000 images with 80%:20% split for train and test respectively.\n",
    "\n",
    "2) Model selection<br/>\n",
    "There are 4 models available. cascaded_encoder_decoder, merged_encoder_decoder, encoder_decoder_with_attention, encoder_decoder_with_transformer.<br>\n",
    "You can select single model, or multiple models separated by comma without space. For example, \"-m transformer,merge\" will run training for the merge model and the transformer model.\n",
    "\n",
    "3) Epoch selection<br/>\n",
    "Number of epochs can be specified then at the end of the given epochs, train weights will be saved for later use in prediction and evaluation. For example, \"-e 30,50,100\" will run total 100 epochs and at the end of 30, 50 and 100 epochs, the weights will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97d9bdb-d686-4073-9fd0-00578ca547f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: icap_tool.py train [OPTIONS]\n",
      "\n",
      "Options:\n",
      "  -d, --dataset TEXT  Dataset name  [default: Flickr30k]\n",
      "  -m, --models TEXT   List of models  [default:\n",
      "                      transformer,attention,merge,cascade]\n",
      "  -e, --epochs TEXT   Save weights after given number of epochs  [default:\n",
      "                      10,20,30,40,50]\n",
      "  --help              Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!python icap_tool.py train --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e33ee48-bd27-4e45-9017-e960da88ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a jupyter notebook, you can call the train function directly\n",
    "from icap_tool import train\n",
    "train('Flickr30k', 'transformer,attention,merge,cascade', '10,20,30,40,50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b086cb-4776-4601-a233-8db1c6e0f8ae",
   "metadata": {},
   "source": [
    "## 'Predict' subcommand\n",
    "\n",
    "Predict command will generation captions for the test images using the pre-trained weights by the 'train' command above.<br/>\n",
    "You can specify the dataset, models and epochs in the same way as the 'train' command.<br/>\n",
    "The generated captions will be saved to files and used for evaluation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f6275f2-6f4d-4192-bbb8-9b7fe0603d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: icap_tool.py predict [OPTIONS]\n",
      "\n",
      "Options:\n",
      "  -d, --dataset TEXT   Dataset name  [default: Flickr30k]\n",
      "  -m, --models TEXT    List of models  [default:\n",
      "                       transformer,attention,merge,cascade]\n",
      "  -e, --epochs TEXT    Load pretrained weights after given number of epochs\n",
      "                       [default: 10,20,30,40,50]\n",
      "  -n, --count INTEGER  Number of images to predict. (0 means all test images)\n",
      "                       [default: 0]\n",
      "  --help               Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!python icap_tool.py predict --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ff83a-e17a-40cc-bb8e-455b2f2e5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a jupyter notebook, you can call the predict function directly\n",
    "from icap_tool import predict\n",
    "predict('Flickr30k', 'transformer,attention,merge,cascade', '10,20,30,40,50', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab4c70-b187-402b-8d04-8e8d28120695",
   "metadata": {},
   "source": [
    "## 'Evaluate' subcommand\n",
    "\n",
    "Evaluate command will calculate the BLEU, ROUGE and METEOR scores by comparing the generated captions by the models and the reference texts given with the original images.<br/>\n",
    "The evaluation scores will be saved to files per-model and per-epoch.<br/>\n",
    "The '-c' option will average out each individual evaluation results and consolidate in a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d7f4c9d-4b43-4490-9587-0cd1b2073a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: icap_tool.py evaluate [OPTIONS]\n",
      "\n",
      "Options:\n",
      "  -d, --dataset TEXT  Dataset name  [default: Flickr30k]\n",
      "  -m, --models TEXT   List of models  [default:\n",
      "                      transformer,attention,merge,cascade]\n",
      "  -e, --epochs TEXT   Load pretrained weights after given number of epochs\n",
      "                      [default: 10,20,30,40,50]\n",
      "  -c, --consolidate   Create consolidated evaluation table\n",
      "  --help              Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!python icap_tool.py evaluate --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef59c6-b60d-4fb7-865c-b5e7a02e870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a jupyter notebook, you can call the evaluate function directly\n",
    "from icap_tool import evaluate\n",
    "evaluate('Flickr30k', 'transformer,attention,merge,cascade', '10,20,30,40,50', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5a9e507-acfe-4e14-b1bd-af050e929aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prebuilt vocabulary ./workspace/Flickr30k-vocab.pkl ... completed\n",
      "Loading caption sequences ./workspace/Flickr30k-caption_sequences.pkl ... completed\n",
      "Loading prebuilt embedding matrix ./workspace/Flickr30k-embedding_matrix_fasttext.pkl ... completed\n",
      "Building image features ./workspace/Flickr30k-vgg16-no_include_top ... 30000/30000 processed. 100% completed\n",
      "Building training model ... WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "completed\n",
      "Loading eval_scores ./workspace/Flickr30k-transformer_model/eval_scores-10.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-transformer_model/eval_scores-20.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-transformer_model/eval_scores-30.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-transformer_model/eval_scores-40.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-transformer_model/eval_scores-50.csv\n",
      "Loading prebuilt vocabulary ./workspace/Flickr30k-vocab.pkl ... completed\n",
      "Loading caption sequences ./workspace/Flickr30k-caption_sequences.pkl ... completed\n",
      "Loading prebuilt embedding matrix ./workspace/Flickr30k-embedding_matrix_fasttext.pkl ... completed\n",
      "Building image features ./workspace/Flickr30k-vgg16-no_include_top ... 30000/30000 processed. 100% completed\n",
      "Building training model ... completed\n",
      "Loading eval_scores ./workspace/Flickr30k-attention_model/eval_scores-10.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-attention_model/eval_scores-20.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-attention_model/eval_scores-30.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-attention_model/eval_scores-40.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-attention_model/eval_scores-50.csv\n",
      "Loading prebuilt vocabulary ./workspace/Flickr30k-vocab.pkl ... completed\n",
      "Loading caption sequences ./workspace/Flickr30k-caption_sequences.pkl ... completed\n",
      "Loading prebuilt embedding matrix ./workspace/Flickr30k-embedding_matrix_fasttext.pkl ... completed\n",
      "Building image features ./workspace/Flickr30k-vgg16-include_top ... 30000/30000 processed. 100% completed\n",
      "Building training model ... completed\n",
      "Loading eval_scores ./workspace/Flickr30k-merge_model/eval_scores-10.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-merge_model/eval_scores-20.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-merge_model/eval_scores-30.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-merge_model/eval_scores-40.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-merge_model/eval_scores-50.csv\n",
      "Loading prebuilt vocabulary ./workspace/Flickr30k-vocab.pkl ... completed\n",
      "Loading caption sequences ./workspace/Flickr30k-caption_sequences.pkl ... completed\n",
      "Loading prebuilt embedding matrix ./workspace/Flickr30k-embedding_matrix_fasttext.pkl ... completed\n",
      "Building image features ./workspace/Flickr30k-vgg16-include_top ... 30000/30000 processed. 100% completed\n",
      "Building training model ... completed\n",
      "Loading eval_scores ./workspace/Flickr30k-cascade_model/eval_scores-10.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-cascade_model/eval_scores-20.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-cascade_model/eval_scores-30.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-cascade_model/eval_scores-40.csv\n",
      "Loading eval_scores ./workspace/Flickr30k-cascade_model/eval_scores-50.csv\n",
      "Writing evaluation result to ./workspace/Flickr30k-eval.csv\n"
     ]
    }
   ],
   "source": [
    "evaluate('Flickr30k', 'transformer,attention,merge,cascade', '10,20,30,40,50', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46fa156f-7ba5-47d7-9287-5cfc78d47426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     metric  epoch  cascade   merge  attention  transformer\n",
      "0    BLEU-1     10   0.5335  0.5261     0.5248       0.5368\n",
      "1    BLEU-1     20   0.5186  0.5243     0.5068       0.5395\n",
      "2    BLEU-1     30   0.5180  0.5097     0.5064       0.5405\n",
      "3    BLEU-1     40   0.5262  0.5024     0.4987       0.5279\n",
      "4    BLEU-1     50   0.5109  0.5159     0.4912       0.5245\n",
      "5    BLEU-2     10   0.2669  0.2681     0.2698       0.2911\n",
      "6    BLEU-2     20   0.2443  0.2651     0.2512       0.2931\n",
      "7    BLEU-2     30   0.2668  0.2532     0.2465       0.2917\n",
      "8    BLEU-2     40   0.2697  0.2544     0.2359       0.2833\n",
      "9    BLEU-2     50   0.2639  0.2572     0.2290       0.2783\n",
      "10   BLEU-3     10   0.0789  0.1002     0.1156       0.1306\n",
      "11   BLEU-3     20   0.0888  0.1049     0.1027       0.1291\n",
      "12   BLEU-3     30   0.1054  0.1036     0.0974       0.1310\n",
      "13   BLEU-3     40   0.1066  0.1067     0.0904       0.1256\n",
      "14   BLEU-3     50   0.1066  0.1071     0.0865       0.1211\n",
      "15   BLEU-4     10   0.0232  0.0358     0.0463       0.0520\n",
      "16   BLEU-4     20   0.0302  0.0434     0.0391       0.0524\n",
      "17   BLEU-4     30   0.0400  0.0440     0.0378       0.0523\n",
      "18   BLEU-4     40   0.0404  0.0481     0.0333       0.0515\n",
      "19   BLEU-4     50   0.0403  0.0487     0.0330       0.0493\n",
      "20  ROUGE-1     10   0.2063  0.2168     0.2246       0.2370\n",
      "21  ROUGE-1     20   0.2018  0.2076     0.2163       0.2359\n",
      "22  ROUGE-1     30   0.2171  0.2028     0.2138       0.2340\n",
      "23  ROUGE-1     40   0.2184  0.2048     0.2102       0.2326\n",
      "24  ROUGE-1     50   0.2157  0.2028     0.2073       0.2310\n",
      "25  ROUGE-2     10   0.0459  0.0498     0.0520       0.0580\n",
      "26  ROUGE-2     20   0.0420  0.0476     0.0477       0.0574\n",
      "27  ROUGE-2     30   0.0493  0.0455     0.0463       0.0575\n",
      "28  ROUGE-2     40   0.0504  0.0464     0.0438       0.0565\n",
      "29  ROUGE-2     50   0.0502  0.0463     0.0426       0.0557\n",
      "30  ROUGE-L     10   0.1873  0.1946     0.2010       0.2111\n",
      "31  ROUGE-L     20   0.1825  0.1880     0.1925       0.2096\n",
      "32  ROUGE-L     30   0.1946  0.1848     0.1904       0.2084\n",
      "33  ROUGE-L     40   0.1953  0.1867     0.1873       0.2069\n",
      "34  ROUGE-L     50   0.1938  0.1880     0.1844       0.2051\n",
      "35   METEOR     10   0.2913  0.3103     0.3262       0.3431\n",
      "36   METEOR     20   0.2829  0.3003     0.3151       0.3461\n",
      "37   METEOR     30   0.3023  0.2949     0.3091       0.3452\n",
      "38   METEOR     40   0.3115  0.2945     0.3028       0.3437\n",
      "39   METEOR     50   0.3077  0.2825     0.2982       0.3436\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('workspace/Flickr30k-eval.csv')\n",
    "print(df.to_string()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4ecef-a95b-4789-904f-7fdafa62768c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "py39_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
